function [model, llh] = mixGaussEmExtension(models, init)
% Perform Extend EM algorithm for fitting the Gaussian mixture model.
% Input: 
%   models: 1 x I(total number of images in each class) data matrix
%           each data in this matrix is a model generated by EM algorithm
%           model.miu: a d-by-k matrix.
%           model.Sigma: a d-by-d-by-k matrix.
%           model.w: a 1-by-k vector.
%   init: M (1 x 1) number of components or label (1 x n, 1<=label(i)<=k) or model structure
% Output:
%   label: 1 x n cluster label
%   model: trained model structure
%          model.mu: a d-by-m matrix.
%          model.Sigma: a d-by-d-by-m matrix.
%          model.w: a 1-by-m vector.
%   llh: loglikelihood

fprintf('Extend EM for Gaussian mixture: running ... \n');
%��models�����struct��ʽ���matrix
%n=I*k
%X��model.miuת��������ά���ǣ�d*n
%W_JK��model.wת��������ά���ǣ�1*n
%SIGMA_JK��model.Sigmaת��������ά���ǣ�d*d*n

[X, W_JK, SIGMA_JK]= transModels(models);

maxiter = 100;
llh = -inf(1,maxiter);
[model] = initialization(X,init);
for iter = 2:maxiter
    R = expectation(X,model,W_JK,SIGMA_JK);
    model = maximization(X,R,W_JK,SIGMA_JK);
    llh(iter) = cal_LLH(X, model);
    disp(['llh of ',num2str(iter),'th iteration is: ',num2str(llh(iter))]);
    if abs(llh(iter)-llh(iter-1)) < 0.05; break; end;
end
llh = llh(2:iter);
end

%% 初始化model
function [model] = initialization(X, init)
[~,n] = size(X);
model = [];

if numel(init) == 1  % random init k
    m = init;
    model.w = ones(1, m) / m;

    rndp = randperm(n);
    if m>n
        index=randi([1,n],1,m);
    else
        index=rndp(1:m);
    end
    model.mu =  X(:,index);

    X_mean = sum(X, 2) ./ n;
    x_minus = bsxfun(@minus,X,X_mean);
    tmp = x_minus*x_minus';
    tmp_pd = PositiveDefiniteTrans(tmp ./ n);
    for k = 1:m
        model.Sigma(:,:,k)=tmp_pd;
    end
    
else
    error('ERROR: init is not valid.');
end
end

%% E步
function [R] = expectation(X, model, W_JK,SIGMA_JK)
mu = model.mu;
Sigma = model.Sigma;
w = model.w;

n = size(X,2);
m = size(mu,2);
R = zeros(n,m);

for i = 1:m
    for j = 1:n
        R(j,i) = w(i) * ((mvnpdf(X(:,j)', mu(:,i)', Sigma(:,:,i)) * exp(-0.5 * trace(Sigma(:,:,i) \ SIGMA_JK(:,:,j)))) ^ W_JK(j));
    end
end
s = sum(R,2);
R = bsxfun(@rdivide,R,s);
NAN_V=find(isnan(R(:,1)));
R(NAN_V,:)=ones(length(NAN_V),m)/m;
end

%% M步
function model = maximization(X,R,W_JK,SIGMA_JK)
[d,n] = size(X);
m = size(R,2);

mu = zeros(d, m);
Sigma = zeros(d,d,m);

w = sum(R, 1)/n;
for k = 1 : m
    w_tmp = R(:,k).*W_JK';
    w_tmp = w_tmp / sum(w_tmp);
    mu(:,k) = X * w_tmp;

    miu_shift = bsxfun(@minus, X, mu(:,k));%d*n
    miu_rep= repmat(reshape(miu_shift,[d,1,n]),1,d);
    sig_tmp = bsxfun(@times,miu_rep,permute(miu_rep,[2,1,3]))+SIGMA_JK;
    sig_tmp = sum(bsxfun(@times,reshape(w_tmp,[1,1,n]),sig_tmp),3);
    Sigma(:,:,k) = PositiveDefiniteTrans(sig_tmp);
end

model.mu = mu;
model.Sigma = Sigma;
model.w = w;
end

%% 计算loglikehood
function llh = cal_LLH(X, model)
[~,n] = size(X);

mu = model.mu;
Sigma = model.Sigma;
w = model.w;

m = size(mu, 2);
tmp = zeros(n,m);
for k= 1:m
    tmp(:,k) = w(k) * mvnpdf(X', mu(:,k)', Sigma(:,:,k));
end
tmp = sum(tmp,2);
tmp = log(tmp);
llh = sum(tmp)/n;
end